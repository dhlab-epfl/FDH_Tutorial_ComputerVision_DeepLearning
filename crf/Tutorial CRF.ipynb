{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Annotating-data\" data-toc-modified-id=\"Annotating-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Annotating data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding-scheme\" data-toc-modified-id=\"Encoding-scheme-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Encoding scheme</a></span><ul class=\"toc-item\"><li><span><a href=\"#IO-encoding\" data-toc-modified-id=\"IO-encoding-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>IO encoding</a></span></li><li><span><a href=\"#IOB-and-IOB2-encoding\" data-toc-modified-id=\"IOB-and-IOB2-encoding-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>IOB and IOB2 encoding</a></span></li><li><span><a href=\"#BILUO-encoding\" data-toc-modified-id=\"BILUO-encoding-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>BILUO encoding</a></span></li><li><span><a href=\"#Conversion-between-encodings\" data-toc-modified-id=\"Conversion-between-encodings-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Conversion between encodings</a></span></li></ul></li><li><span><a href=\"#Code-part\" data-toc-modified-id=\"Code-part-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Code part</a></span></li></ul></li><li><span><a href=\"#Training-a-CRF\" data-toc-modified-id=\"Training-a-CRF-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training a CRF</a></span><ul class=\"toc-item\"><li><span><a href=\"#With-little-annotations\" data-toc-modified-id=\"With-little-annotations-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>With little annotations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-and-splitting-data\" data-toc-modified-id=\"Loading-and-splitting-data-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Loading and splitting data</a></span></li><li><span><a href=\"#Generate-features-and-train\" data-toc-modified-id=\"Generate-features-and-train-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Generate features and train</a></span></li></ul></li><li><span><a href=\"#With-more-annotations\" data-toc-modified-id=\"With-more-annotations-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>With more annotations</a></span></li></ul></li><li><span><a href=\"#Predicting-for-new-data\" data-toc-modified-id=\"Predicting-for-new-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Predicting for new data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this tutorial, we will train a [Conditional Random Field (CRF)](https://en.wikipedia.org/wiki/Conditional_random_field) on a dataset of the owner field of the 1808 napoleonian _Sommarioni_ that were manually transcripted.\n",
    "\n",
    "![sommarioni](https://images.center/iiif_sommarioni/reg5-0103/1047,5697,1485,775/500,/0/default.jpg) becomes two entries:\n",
    "1. `Città di Venezia, di provenenienza della sopressa Scola di Santa Maria della Carità`\n",
    "2. `Raspi Giovanni Francesco quondam Giovanni Maria`\n",
    "\n",
    "The goal of this tutorial is to start from these entries and extract all the first names, last names, ecclesiastical entities and governmental entitie. This will be done using CRFs.\n",
    "\n",
    "A CRF is a discriminative probabilistic graphical model. It thus relies on modeling an input sequence with a graph and tries to estimate the conditional distribution $p(y|\\mathbf{x})$. A good introduction can be found in [this paper](https://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf).\n",
    "\n",
    "Compared to a discrete classifier, such as logistic regression which predicts each sample indepentently, a CRF can process a sequence of samples and take the context into account. It predicts a seqence of labels for an input sequence. This makes it particularly useful for labelling or parsing sequential data such as text.\n",
    "\n",
    "CRF take as input a sequence of features which are computed for each input token. These features can be hand-crafted has it will be done in this tutorial. The advantage is that one can use prior knowledge on the data, without the need to write a rule based system.\n",
    "\n",
    "In state of the art in named entity recognition the features come from a deep learning model, but a CRF is still often used for making the actual prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating data\n",
    "\n",
    "Since CRF is a supervised machine learning model, it needs training data.\n",
    "\n",
    "The text must be tokenized into words. In our example, each line is considered as a sequence, but it is possible to use other level, such as the paragraph, article or even the page.\n",
    "\n",
    "The most common tagging scheme is called [Inside-Outside-Beginning (IOB for sort)](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)). \n",
    "\n",
    "We first define a set of tags of tokens we want to be able to predict:\n",
    "- `FNAME` for the first name.\n",
    "- `LNAME` for the last name.\n",
    "- `ECC` for an ecclesiastical entity.\n",
    "- `GOV` for a governmental entity.\n",
    "\n",
    "The idea is then to prefix the tags with information about the role of the chunk inside its class.\n",
    "\n",
    "### Encoding scheme\n",
    "There exists several way to encode the chunk information, we will review briefly the principal ones, more information can be found on this [blog post](https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/).\n",
    "#### IO encoding\n",
    "This is the simplest encoding, each token is either _in_ an entity, shown with the `I` prefix, e.g. `I-FNAME`, or out of any entity `O`. The disadvantage is that the encoding cannot reresent two entities next to each other since there is no boundary prefix.\n",
    "\n",
    "An example tagging is :\n",
    "\n",
    "|**Tagging**|Raspi|Francesco|e|le|Ministerio|della|finanzia|Senato|quondam|\n",
    "|-----------|-----|---------|-|--|----------|-----|--------|------|-------|\n",
    "|**IO**|I-LNAME|I-FNAME|O|O|I-GOV|I-GOV|I-GOV|I-GOV|O|\n",
    "\n",
    "\n",
    "It is impossible to distinguish between the two `GOV` entitites.\n",
    "\n",
    "#### IOB and IOB2 encoding\n",
    "This encoding now uses two prefixes for the entities, `B` indicates the beginning of an entity and `I` is the continuation of an entity. For single entities, in `IOB` they are tagged as `I` and in `IOB2` as `B`:\n",
    "\n",
    "|**Tagging**|Raspi|Francesco|e|le|Ministerio|della|finanzia|Senato|quondam|\n",
    "|-----------|-----|---------|-|--|----------|-----|--------|------|-------|\n",
    "|**IO**|I-LNAME|I-FNAME|O|O|I-GOV|I-GOV|I-GOV|I-GOV|O|\n",
    "|**IOB**|I-LNAME|I-FNAME|O|O|B-GOV|I-GOV|I-GOV|I-GOV|O|\n",
    "|**IOB2**|B-LNAME|B-FNAME|O|O|B-GOV|I-GOV|I-GOV|B-GOV|O|\n",
    "\n",
    "The advantage of IOB2 is that is is able to separate between the two `GOV` entities.\n",
    "\n",
    "Note that IOB is also sometimes called BIO.\n",
    "\n",
    "\n",
    "#### BILUO encoding\n",
    "Two more prefixes are introduced, `L` which denotes the end of an entity and `U` which denotes a unique token for an entity.\n",
    "\n",
    "|**Tagging**|Raspi|Francesco|e|le|Ministerio|della|finanzia|Senato|quondam|\n",
    "|-----------|-----|---------|-|--|----------|-----|--------|------|-------|\n",
    "|**IO**|I-LNAME|I-FNAME|O|O|I-GOV|I-GOV|I-GOV|I-GOV|O|\n",
    "|**IOB**|I-LNAME|I-FNAME|O|O|B-GOV|I-GOV|I-GOV|I-GOV|O|\n",
    "|**IOB2**|B-LNAME|B-FNAME|O|O|B-GOV|I-GOV|I-GOV|B-GOV|O|\n",
    "|**BILOU**|U-LNAME|U-FNAME|O|O|B-GOV|I-GOV|L-GOV|U-GOV|O|\n",
    "\n",
    "Note that BILUO is also sometimes BIOES or BMEWO.\n",
    "\n",
    "#### Conversion between encodings\n",
    "\n",
    "It is posibble to convert between encodings, the following table summarizes the possible conversions:\n",
    "\n",
    "|Source/Target|IO|IOB|IOB2|BILUO|\n",
    "|-------------|--|---|----|-----|\n",
    "|IO           |✓ |⨯  |⨯   |⨯    |\n",
    "|IOB          |✓ |✓  |⨯   |⨯    |\n",
    "|IOB2         |✓ |✓  |✓   |✓    |\n",
    "|BILUO        |✓ |✓  |✓   |✓    |\n",
    "\n",
    "IO and IOB encoding loses information, since it is not possible to cleanly separate glued entities of the same tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code part\n",
    "\n",
    "We first start with some imports, some functions are defined inside the `utils` folder and will be referred as functions coming from utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "import spacy\n",
    "\n",
    "from utils.export import export_to_excel\n",
    "from utils import tags_format\n",
    "from utils.features import generate_features\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "nlp = spacy.load('it_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define what we want to tag, we use the same set that we defined in part 2:\n",
    "- `FNAME` for the first name.\n",
    "- `LNAME` for the last name.\n",
    "- `ECC` for an ecclesiastical entity.\n",
    "- `GOV` for a governmental entity.\n",
    "\n",
    "We will use the IOB2 encoding because it offers better separation between entites than IO and IOB, with only two prefixes (vs 4 in BILUO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    'FNAME',\n",
    "    'LNAME',\n",
    "    'ECC',\n",
    "    'GOV'\n",
    "]\n",
    "tags_prefixed = [\"-\".join([prefix, tag]) for tag in tags for prefix in 'IB']\n",
    "tags_prefixed.append('O')\n",
    "print(tags_prefixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data of the owners of *Sommarioni*, it is a CSV with two columns, the index of the entry which is an internal identifier to keep track of it and a text column which contains the text of each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_annotate = pd.read_csv('./propr_to_annotate.csv', index_col=0)\n",
    "df_to_annotate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tokenize the text into words and create a dataframe with the idx and token idx as index, the full text of the entity and the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tokenize the text into words\n",
    "df_tokens = df_to_annotate['text'].apply(lambda text: [\n",
    "    str(token.text) for token in nlp(text, disable=['parser', 'tagger', 'ner'])])\n",
    "\n",
    "# We stack the tokens to get a new token index\n",
    "df_tokens = df_tokens.apply(pd.Series).rename_axis('tok_idx', axis=1).stack().to_frame('token')\n",
    "\n",
    "# Finally, we join with the previous dataframe to get the fulltext column\n",
    "df_tokens = df_tokens.join(df_to_annotate)\n",
    "df_tokens = df_tokens[['text', 'token']]\n",
    "display(df_tokens.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use an util function that takes as input a dataframe with a 'text' and a 'token' column and a list of tags and generates an excel for annotating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_excel(df_tokens, './propr_to_annotate.xlsx', tags=tags_prefixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is annotated, we can read the excel and train a CRF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a CRF\n",
    "\n",
    "Once enough data has been annotated, it is possible to train a CRF.\n",
    "\n",
    "We first start by using our annotated data and then try with more annotated data.\n",
    "\n",
    "### With little annotations\n",
    "\n",
    "#### Loading and splitting data\n",
    "We first load the data from the excel and make sure tags are uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_annotations_small = pd.read_excel('./propr_to_annotate.xlsx', index_col=[0,1])\n",
    "df_annotations_small['tag'] = df_annotations_small['tag'].str.upper()\n",
    "df_annotations_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find all the entries that are completely annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_annotated = (df_annotations_small['tag'].fillna(False) # Fill all NaN with False\n",
    "                .apply(lambda x: True if x else False) # Make filled tag True\n",
    "                .reset_index().groupby('idx') # Transform into dataframe and group by entry\n",
    "                .apply(lambda x: all(x['tag']))) # Check that all tags are filled\n",
    "df_annotations_small = df_annotations_small.loc[is_annotated[is_annotated].index] # Only keep entries with all tags filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then transform the annotation into list of tokens and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_small = df_annotations_small.groupby(level=[0])[['token', 'tag']].agg(list).rename(columns={'token': 'tokens', 'tag': 'tags_iob2'})\n",
    "annotations_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate other tagging scheme using some utils functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "annotations_small['tags_io'] = annotations_small['tags_iob2'].apply(tags_format.iob2_to_io)\n",
    "annotations_small['tags_biluo'] = annotations_small['tags_iob2'].apply(tags_format.iob2_to_biluo)\n",
    "annotations_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now split our data into train and test sets. We will keep them fixed as we add more features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(annotations_small.index, test_size=0.5)\n",
    "print(f\"There is {len(train_idx)} training samples and {len(test_idx)} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate features and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define some features for each token. Usually, we define features for the word and for its surrounding words.\n",
    "\n",
    "Features are often morphological properties of the token, such as the fact that it is in upper case or titled.\n",
    "\n",
    "You can also make use of a list of venetian family names and common italian surnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of venetian famility names\n",
    "family_names = set(pd.read_csv('venetian_names.csv', sep='\\t')['famille'].tolist())\n",
    "\n",
    "# List of italian surnames\n",
    "surnames = set(pd.read_csv('italian_surnames.txt', header=None)[0].str.lower().values.tolist())\n",
    "\n",
    "# Features for the current word\n",
    "default_features = {\n",
    "        'bias': None,\n",
    "        'word.lower': lambda word: word.lower(),\n",
    "        # TODO add other features\n",
    "    }\n",
    "# Features to be computed for surrounding words\n",
    "default_surrounding_features = {\n",
    "        # TODO add features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate the features using another utils function that takes as input the tokens, the current word features, the number of surrounding words and the surrounding word features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_venetian(tokens):\n",
    "    return generate_features(tokens,\n",
    "                      features=default_features,\n",
    "                      n_surrounding=1,\n",
    "                      surrounding_features=default_surrounding_features)\n",
    "\n",
    "annotations_small['features'] = annotations_small['tokens'].apply(generate_features_venetian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, features = annotations_small[['tokens', 'features']].iloc[0].values\n",
    "print(tokens[0])\n",
    "print(features[0])\n",
    "print()\n",
    "print(tokens[2])\n",
    "print(features[2])\n",
    "print()\n",
    "print(tokens[-1])\n",
    "print(features[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and last words have the additional BOS and EOS features, but miss the previous and next word features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = annotations_small.loc[train_idx]\n",
    "test = annotations_small.loc[test_idx]\n",
    "\n",
    "# TODO you can change the encoding scheme to see if it performs diffently\n",
    "tags_col = 'tags_io'\n",
    "\n",
    "X_train = train['features'].values\n",
    "y_train = train[tags_col].values\n",
    "X_test = test['features'].values\n",
    "y_test = test[tags_col].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can use [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io/en/latest/) which is a scikit-learn wrapper of [python-crfsuite](https://github.com/scrapinghub/python-crfsuite) which itself is a wrapper of the C++ [CRFsuite](https://github.com/chokkan/crfsuite). The advantage of using this wrapper is that we can use the scikit-learn api with our CRF.\n",
    "\n",
    "We create a CRF object with sensible default that can be tweaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=3e-1,\n",
    "    c2=1e-4,\n",
    "    max_iterations=1000,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can evaluate it, the most common metric considered is the weighted averaged f1-score.\n",
    "\n",
    "But we can also use the `sklearn-crfsuite.metric.classification` report to have a complete classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "print(f\"Weighted f1_score {metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels):.3f}\")\n",
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to take a look at which features are the most discriminative for a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "        \n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(10))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-10:][::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load more annotations and test how well our model generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = pd.read_excel('./propr_annotated.xlsx', index_col=[0,1])\n",
    "df_annotations['tag'] = df_annotations['tag'].str.upper()\n",
    "is_annotated = (df_annotations['tag'].fillna(False) # Fill all NaN with False\n",
    "                .apply(lambda x: True if x else False) # Make filled tag True\n",
    "                .reset_index().groupby('idx') # Transform into dataframe and group by entry\n",
    "                .apply(lambda x: all(x['tag']))) # Check that all tags are filled\n",
    "df_annotations = df_annotations.loc[is_annotated[is_annotated].index] # Only keep entries with all tags filled\n",
    "annotations = df_annotations.groupby(level=[0])[['token', 'tag']].agg(list).rename(columns={'token': 'tokens', 'tag': 'tags_iob2'})\n",
    "annotations['tags_io'] = annotations['tags_iob2'].apply(tags_format.iob2_to_io)\n",
    "annotations['tags_biluo'] = annotations['tags_iob2'].apply(tags_format.iob2_to_biluo)\n",
    "\n",
    "annotations['features'] = annotations['tokens'].apply(generate_features_venetian)\n",
    "print(f\"There is {len(annotations)} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "X_large = annotations['features'].values\n",
    "y_large = annotations[tags_col].values\n",
    "\n",
    "y_pred = crf.predict(X_large)\n",
    "print(f\"Weighted f1_score {metrics.flat_f1_score(y_large, y_pred, average='weighted', labels=labels):.3f}\")\n",
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_large, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO add more features or change the encoding scheme to see how it changes the results and check how it affects the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With more annotations\n",
    "\n",
    "We copy paste from the last part this time loading the bigger annotations file and without explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_annotations = pd.read_excel('./propr_annotated.xlsx', index_col=[0,1])\n",
    "df_annotations['tag'] = df_annotations['tag'].str.upper()\n",
    "is_annotated = (df_annotations['tag'].fillna(False) # Fill all NaN with False\n",
    "                .apply(lambda x: True if x else False) # Make filled tag True\n",
    "                .reset_index().groupby('idx') # Transform into dataframe and group by entry\n",
    "                .apply(lambda x: all(x['tag']))) # Check that all tags are filled\n",
    "df_annotations = df_annotations.loc[is_annotated[is_annotated].index] # Only keep entries with all tags filled\n",
    "annotations = df_annotations.groupby(level=[0])[['token', 'tag']].agg(list).rename(columns={'token': 'tokens', 'tag': 'tags_iob2'})\n",
    "annotations['tags_io'] = annotations['tags_iob2'].apply(tags_format.iob2_to_io)\n",
    "annotations['tags_biluo'] = annotations['tags_iob2'].apply(tags_format.iob2_to_biluo)\n",
    "\n",
    "train_idx, test_idx = train_test_split(annotations.index, test_size=0.2)\n",
    "print(f\"There is {len(annotations)} annotations\")\n",
    "print(f\"There is {len(train_idx)} training samples and {len(test_idx)} test samples.\")\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same code as before to generate the features, you can copy-paste your previously defined features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_col = 'tags_iob2'\n",
    "\n",
    "# List of venetian famility names\n",
    "family_names = set(pd.read_csv('venetian_names.csv', sep='\\t')['famille'].tolist())\n",
    "\n",
    "surnames = set(pd.read_csv('italian_surnames.txt', header=None)[0].str.lower().values.tolist())\n",
    "\n",
    "# Features for the current word\n",
    "default_features = {\n",
    "        'bias': None,\n",
    "        'word.lower': lambda word: word.lower(),\n",
    "        # TODO add other features\n",
    "    }\n",
    "# Features to be computed for surrounding words\n",
    "default_surrounding_features = {\n",
    "        # TODO add features\n",
    "}\n",
    "\n",
    "def generate_features_venetian(tokens):\n",
    "    return generate_features(tokens,\n",
    "                      features=default_features,\n",
    "                      n_surrounding=1,\n",
    "                      surrounding_features=default_surrounding_features)\n",
    "\n",
    "annotations['features'] = annotations['tokens'].apply(generate_features_venetian)\n",
    "\n",
    "train = annotations.loc[train_idx]\n",
    "test = annotations.loc[test_idx]\n",
    "X_train = train['features'].values\n",
    "y_train = train[tags_col].values\n",
    "X_test = test['features'].values\n",
    "y_test = test[tags_col].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train again the model and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=5e-1,\n",
    "    c2=1e-3,\n",
    "    max_iterations=1000,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train);\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "print(f\"Weighted f1_score {metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels):.3f}\")\n",
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the states features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common(10))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf.state_features_).most_common()[-10:][::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_predict = pd.read_csv('./propr_to_predict.csv', index_col=0)\n",
    "df_to_predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize it and generate the tokens features we previously defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tokenize the text into words\n",
    "df_tokens = df_to_predict['text'].apply(lambda text: [\n",
    "    str(token.text) for token in nlp(text, disable=['parser', 'tagger', 'ner'])]).to_frame('tokens')\n",
    "\n",
    "# We generate features for the tokens\n",
    "df_tokens['features'] = df_tokens['tokens'].apply(generate_features_venetian)\n",
    "\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the crf to predict the features into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens['tags'] = crf.predict(df_tokens['features'].values)\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can transform the data frame to be in the long form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate long form data frames for tokens and tags\n",
    "tokens = df_tokens['tokens'].apply(pd.Series).rename_axis('tok_idx', axis=1).stack().to_frame('token')\n",
    "tags = df_tokens['tags'].apply(pd.Series).rename_axis('tok_idx', axis=1).stack().to_frame('tag')\n",
    "\n",
    "# We join the tokens, the tags and the original text\n",
    "df_export = tokens.join(tags).join(df_to_predict)[['text', 'token', 'tag']]\n",
    "df_export.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again export it to an xlsx file and verify the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_excel(df_export, 'propr_to_verify.xlsx', tags=tags_prefixed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
