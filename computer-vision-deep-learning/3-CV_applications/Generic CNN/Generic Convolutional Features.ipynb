{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we show how generic Convolutional Neural Network (CNN) features can be extracted and used.\n",
    "\n",
    "We will be using the keras library which is a full-fledged Deep-Learning library, but is extremely simple to use for basic feature extractions.\n",
    "\n",
    "This example is based on https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting CNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Inception network which is a neural network architecture developped by Google (http://arxiv.org/abs/1512.00567)\n",
    "from tensorflow.keras.applications import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the architecture, with pre-trained parameters from ImageNet\n",
    "# but without the final (top) layer of object classification\n",
    "# and averaging (avg) the output feature maps to get a constant size feature vector\n",
    "model = inception_v3.InceptionV3(include_top=False, pooling='avg', weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning system usually works with batch of images instead of a single one for efficiency. Here we care about simplicity so below is a helper function for making a single image go through the network. Understanding these details are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img):\n",
    "    x = np.expand_dims(img.astype(np.float32), axis=0)  # Make a batch of 1 image by expanding the first dimension\n",
    "    x = inception_v3.preprocess_input(x)  # Preprocess the input image ([0,255] range to [-1,+1])\n",
    "    return model.predict(x)[0]  # Make the batch goes through the network and convert the batch back to single image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to extract features from an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = io.imread('images/madonna/0158f5ff498a4f84b0c6130b3b73f713.jpg')\n",
    "plt.imshow(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features for the image. The result is a 2048-d vector of numbers\n",
    "features_test = extract_features(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_test.shape)\n",
    "print(features_test)\n",
    "plt.plot(features_test)\n",
    "plt.xlabel('Feature index')\n",
    "plt.ylabel('Feature magnitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was disappointing... Looks like a random list of 2048 numbers.\n",
    "\n",
    "In fact, these generic representations are not powerful by themselves. But they are quite efficient when one needs to work in the space of images, which we are going to do next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple classifier from CNN descriptors\n",
    "\n",
    "In this section, we will try to perform the task of trying to detect 'Virgin and Child'-like compositions in paintings (the usefulness of such a task does not really matter for the purpose of that exercise).\n",
    "\n",
    "The files are in the `images` folder if you want to have a look at it. One folder (madonna) contains the target (or positive) images that we want our system to detect, and the second (random) contains random images. A third folder (test) contains images which are not used during training to test our system at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Gather the image files in each directory\n",
    "filenames_madonna = glob('images/madonna/*.jpg')\n",
    "print(\"Number of training positive files : {}\".format(len(filenames_madonna)))\n",
    "filenames_random = glob('images/random/*.jpg')\n",
    "print(\"Number of training negative files : {}\".format(len(filenames_random)))\n",
    "filenames_test = glob('images/test/*.jpg')\n",
    "print(\"Number of test files : {}\".format(len(filenames_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably have noticed that only 17 images are used to show the system the concept we are looking for, and 50 random images for the opposite concept. The power of these generic CNN representation is that they have already a high-level understanding of the visual information and can generalize quite easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "# Extracting features for the positive images\n",
    "for filename in tqdm(filenames_madonna, desc='Computing features for madonna images'):\n",
    "    img = io.imread(filename)\n",
    "    # Add the CNN features to the list\n",
    "    features.append(extract_features(img))\n",
    "    # Add the positive label to the list\n",
    "    labels.append(1.)\n",
    "# Extracting features for the negative images\n",
    "for filename in tqdm(filenames_random, desc='Computing features for random images'):\n",
    "    img = io.imread(filename)\n",
    "    # Add the CNN features to the list\n",
    "    features.append(extract_features(img))\n",
    "    # Add the negative label to the list\n",
    "    labels.append(0.)\n",
    "features = np.stack(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have processed 69 images, extracting all the features that we stacked in a 2d array whose shape is then [NumberImages, FeatureDimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we recorded the training labels (+1 for positive image, 0 for negative image) for the 69 images. The first 17 are the positives and the next 52 are the negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate a classifier that will predict a probability\n",
    "classifier = SVC(probability=True, kernel='linear')\n",
    "# Train the binary classifier from the extracted features and telling it which images are the good ones with the labels\n",
    "classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a classifier that 'supposedly' has learned to distinguish what we want, so let us try it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each image in the test data\n",
    "for filename in tqdm(filenames_test, desc=\"Predicting\"):\n",
    "    # Load the image\n",
    "    img = io.imread(filename)\n",
    "    # Extract the features\n",
    "    feat = extract_features(img)\n",
    "    # Get the probabilities for the computed features\n",
    "    probs = classifier.predict_proba(np.expand_dims(feat, axis=0))[0]\n",
    "    prob_madonna = probs[1]  # probs[0] would be the probability of negative class, probs[1] is for the positive class\n",
    "    # Display the results\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Probability : {:.01f}%\".format(100*prob_madonna))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see the result is not perfect, but remember the system was just trained with less than 70 images!\n",
    "\n",
    "Bottom line is that CNN features are a good first thing to try for a generic vision task. It is now easy to use a pretrained system and leverage the power of the computer-vision community.\n",
    "\n",
    "However, always remember that these features do no mean much by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For people who are bored, can you compute an embedding from the computed features?\n",
    "# have a look at http://scikit-learn.org/stable/modules/manifold.html if you do not know where to start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
